---
title: "Why Do We Do Statistics?"
author: "Leighton Pritchard"
date: "2021 Presentation"
output: 
  bookdown::html_document2:
    toc: true
    toc_float:
      toc_collapsed: false
    number_sections: true
    css: "css/rmd_style.css"
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library("DT")
library("ggplot2")

# BG color for plots - should match .figure and .caption classes in rmd_style.css
figbg = "whitesmoke"
```

# Introduction

<div id="summary">
- Statistics is a science that aims to solve researchers' problems by analysing data
  - The value of statistics is in giving quantitative answers to questions
  - Whether the question is well-defined, or the statistical method well-chosen, is another matter.
- This workshop aims to help you critically interpret literature and choose approaches for your own research, but does not focus on theory or mathematical formalism
- **The workshop is presented in a flipped style.**
  - You should read through the materials, attempt the exercises and interactive sections, and bring questions for discussion in the live online session.
- The interactive sessions can be attempted online, but you could also use `RStudio` Desktop, installed on your own computer.
- **The live online session is for course material only. Technical questions about running `RStudio` will *not* be addressed at the live online session. Please email the workshop presenters beforehand if you need help with technical issues.**
</div>

> A major point, on which I cannot yet hope for universal agreement, is that our focus must be on questions, not models [â€¦] Models can - and will - get us into deep troubles if we expect them to tell us what the unique proper questions are. - J.W. Tukey (1977)



# Why Do We Do Statistics?

*Statistics* is a science that aims to solve problems and answer questions mathematically, by representing and analysing *data*. Statistics is applicable to any field that collects data, including medicine, biology, chemistry, engineering, and social sciences. The nature of data can be complex, and there are always some practical questions that can only be answered by the application of statistical methods and *models* to that data.

We are often concerned with the "truth" of some answer to our scientific questions: does the drug, when administered, produce a measurable effect? How much active compound is there in a mass-produced capsule? But statistical models are neither true nor false. They are tools, or *constructs*, that we have developed to enable us to answer our questions. But they are only tools. They are powerful tools, for sure, able to carry out exact calculations (nearly) the same way every time. But they are not *wise*. They do not know when the context is inappropriate. They do not know the meaning of your question. They are procedures that operate on data. They will give you answers, but they may not give you good answers, or the answer to the question you thought you were asking.

Many courses will teach a "Hypothesis Testing Decision Tree," or something similar (see below). 

![A Hypothesis Testing Decision Tree, linked from Pirk *et al.* (2013) *J. Apic. Res.* doi:10.3896/IBRA.1.52.4.13](https://www.researchgate.net/profile/Christian-Pirk/publication/256303889/figure/fig5/AS:392777901854729@1470656957861/A-basic-decision-tree-on-how-to-select-the-appropriate-statistical-test-is-shown_W640.jpg)

These flowcharts do not encourage much in the way of thought about the underlying structures, models and processes of these methods, and imply that the small collection of tests will do for all circumstances (which is absolutely not true). They tend not to present any conceptual framework that helps researchers through the inevitable cases where the formulaic application of these flowcharts is inappropriate for the question being asked of the research. These classical statistical methods tend to be inflexible and fragile: unable to adapt to novel contexts not met by their assumptions; and failing in unpredictable ways when they meet those contexts.

In this workshop, we aim to give you some insight and tools to help you understand common statistical methods you might encounter, why they were used and - occasionally - why they perhaps should not have been used. One of our goals is to encourage you to look past a rote application of "The Null Ritual" (as coined in [Gigerenzer (2004) *J. Socio-Econ.* doi:10.1016/j.socec.2004.09.033](https://doi.org/10.1016/j.socec.2004.09.033)):

1. Set up a statistical null hypothesis of "no mean difference" or "zero correlation." Don't specify the predictions of your research hypothesis or of any alternative substantive hypotheses.
2. Use 5% as a convention for rejecting the null. If significant, accept your research hypothesis. Report the result as *p*<0.05, *p*<0.07, or *p*<0.001 (whichever comes next to the obtained *p*-value).
3. Always perform this procedure.

<div id="warning">
You will see this ritual practised mechanically in many published papers. It is a bad habit in science. 

The Null Ritual is frequently observed in publications where Null Hypothesis Significance Testing (NHST) has been performed. We will explore NHST in more detail, and describe alternative approaches to both NHST and the Null Ritual during the workshop.
</div>

<div id="note">
Ths workshop does not attempt to teach you the "right" way to do statistics, in the sense of "use tool X for problem Y". Fundamentally, there is no recipe for this, and there is no shortcut for understanding the question you wish to ask with your experiment, and whether your chosen statistical method will actually answer that question.

We are instead trying to introduce some statistical thinking, in terms of understanding the bases of common statistical tools, their applications, and their limitations.

**Whichever tool you use, understanding your own experiment and data or, if you're critically assessing someone else's research, understanding their experiment and data, should be the foundational starting point for all analyses.**
</div>

<details>
  <summary>Click to toggle some advice about learning more about statistics</summary>
If I were to recommend three tools to learn, and a book to learn them from, I would point you to Richard McElreath's *Statistical Rethinking*, which covers:

1. Bayesian data analysis
2. Multilevel models
3. Model comparison using information criteria
</details>


# What Does Statistics Do?

When researchers carry out experiments, they typically collect *data* for that experiment. This data usually involves at least two *measurements* of each of a number of individuals (or *experimental units*). Sometimes "individuals" means actual individual people, but it might also mean individual medications, groups of people, or something else, depending on context.

Often, one of the measurements is treated as a *response variable* - some kind of *outcome* that the researcher is interested in. For instance, if we were to measure pupil dilation as a result of administering a medication, we would treat the amount of dilation as the *response variable*. In terms of experimental design, the researcher is attempting to determine how the response variable *depends on* - or is *explained by* the other measured variables. The response variable is sometimes called the *dependent variable* because it *depends on* the other variables in the experiment.

The measured variables that might *explain* the behaviour of the response variable (i.e. those variables on which the dependent variable might depend) are called *explanatory variables*. For instance, the pupil dilation in our example might depend on: the amount of drug given; the sex of the patient; the age of the patient; and so on. Explanatory variables in an experiment might be measured (i.e. properties of the individual that are observed, like patient sex) or assigned (e.g. giving a drug or a placebo to different groups of individuals). As the explanatory variables are considered not to depend on the other variables in the experiment, they may be called *independent variables*.

```{r plant-growth, echo=FALSE}
datatable(PlantGrowth, width=400,
          caption="Table 1: Data from an experiment assessing the effect of two treatment conditions on crop yield, in comparison to a 'control' (no treatment). `weight` is the response variable, `group` is the (assigned) explanatory variable.")
```

### Understanding and Prediction

Statistics enables two key types of researchers' questions to be answered: *understanding* questions, and *prediction* questions.

- *understanding* questions might take the form of determining whether changing the value of an *explanatory* variable affects the value of the *response* variable. The question might be asking whether there is a change at all, or how much of a change there is.
- *prediction* questions also need to determine a similar relationship between *explanatory* and *response* variables, but they then seek to use those relationships to *predict* responses of individuals on the basis of explanatory variables that might be measured in the future.

### Models

In all questions, whether for understanding or prediction, we rely on *modelling* the behaviours of the variables we measure, and also the interactions between those variables - such as the way that explanatory variables influence the response variable value. Modelling is a broad, complex field, but for the purpose of this workshop we can consider *models* to be simplified representations of data, and of the relationships between data. In this workshop, we'll use two main kinds of model.

Firstly, although we measure data in our experiments, for some statistical approaches we do not use the data directly to calculate output. Instead, we use the data to find *parameters* of a *distribution* (e.g. *mean* and *standard deviation*), such as a Normal or Poisson distribution. This distribution is an idealised *model* of how the data behaves, and it simplifies our work to use this model to represent our data, instead of the actual individdual values of the data 

<div id="note">
If you have ever performed a *t*-test or similar statistical analysis, based on the means and standard deviations of two datasets, you were doing exactly this!

The means and standard deviations were parameters that described a model of your data, in place of using the actual measurements themselves. These would then have been used to answer an *understanding* question: "is there evidence that two distributions are different?"
</div>

A key concern in statistics is ensuring that the choice of model distribution to represent an experimental dataset is appropriate, and we will spend some time in the workshop notebooks exploring this.

```{r model-normal, echo=FALSE, fig.cap="Two hundred measured data values (histogram) and the corresponding Normal distribution model (curve) of those values. The modelled distribution is entirely described by the parameters: mean=10 (Âµ) and standard deviation=3 (Ïƒ). We can see here that the frequency of real data values don't match the curve exactly; the model simplifies our representation of the 'noisy' data. The dashed line represents the mean value of the model, and the dotted lines represents values at +/- one and two standard deviations from the mean."}
mu = 10                          # define mean of distribution
sd = 3                           # define standard deviation of distribution
breaks = seq(0, 20, by=0.4)      # set up the breakpoints between bars in the histogram

dfm_hist = data.frame(vals=rnorm(200, mean=mu, sd=sd))               # create a data frame

p = ggplot(dfm_hist, aes(x=vals))                                    # set up the ggplot with data
p = p + geom_histogram(aes(y=..density..),                           # add a historgram layer
                       breaks=breaks,
                       fill="cornflowerblue")
p = p + stat_function(fun=dnorm, args=list(mean=mu, sd=sd),          # add a layer with the Normal curve 
                      geom="line")
p = p + annotate("segment",                                          # show the mean as a dashed line
                 x=mu, xend=mu, y=0, yend=0.2,
                 colour="darkorange1", size=1, linetype="dashed")
p = p + annotate("segment",                                          # show standard deviations as dotted lines
                 x=c(mu-2*sd, mu-sd, mu+sd, mu+2*sd),
                 xend=c(mu-2*sd, mu-sd, mu+sd, mu+2*sd),
                 y=c(0, 0, 0, 0), yend=c(0.2, 0.2, 0.2, 0.2),
                 colour="goldenrod", size=1, linetype="dotted")
p = p + annotate("text",                                             # annotate the lines
                 x=c(mu-2*sd, mu-sd, mu, mu+sd, mu+2*sd),
                 y=c(0.21, 0.21, 0.21, 0.21, 0.21),
                 colour="darkorange3",
                 label=c("Âµ - 2Ïƒ", "Âµ - Ïƒ", "Âµ", "Âµ + Ïƒ", "Âµ + 2Ïƒ"))
p = p + xlim(mu - 3 * sd, mu + 3 * sd)                               # set x-axis limits
p = p + xlab("measured variable") + ylab("frequency")                # add axis labels
p = p + theme(plot.background = element_rect(fill = figbg,           # colour background
                                             color = figbg))
p                                                                    # show plot
```


Secondly, the relationship between explanatory and response variables is described by a simplified representation of that relationship: this is also a *model*. For example, if we fit a linear regression between measured values of "plant height" (response variable) and "hours of sunlight received" (explanatory variable), the line of best-fit is a *model* of the relationship between those variables. The inferred values of *intercept* and *gradient* (or *slope*) for this line are *parameters* of that model.

Much of statistical analysis is about estimating the values of parameters for these models, and exploring the implications of the resulting models.

<div id="note">
Please take some time to explore a statistical distribution interactively in your browser, at the link below.

- [1a. Exploring a Statistical Distribution **INTERACTIVE SESSION**](https://leighton-pritchard.shinyapps.io/01a-sampling/)

This will allow you to explore the way in which the parameters of a statistical distribution change its shape, and alter how well it represents your measured data.
</div>

<div id="warning">
**BEWARE OF HISTOMANCY!**

One strategy for choosing a distribution to represent your data is to plot a histogram of that data and, by looking at it, choose an appropriately-shaped distribution. In his book *Statistical Rethinking*, Richard McElreath refers to this as "Histomancy", by analogy to haruspicy, necromancy and other fake magic arts that claimed to disclose hidden truths by looking at entrails or speaking with the dead.

As you will have seen from the interactive session, even the data that was drawn specifically from a Normal Distribution did not always look like a Normal Distribution when presented as a histogram. In general, it may very well be that a given choice of distribution is good or poor, but it cannot determined by examining histograms, or by guesswork. We need to apply sound principles to our choice of statistical methods, and this workshop will attempt to give you some intuitions about those choices, for your own work, and to help approach others' work critically.
</div>

## Why Are We Doing This Workshop

The emphasis of this workshop is to introduce common statistical tools, and help you develop an insight into how they work and when they should, and perhaps also when they ought not to, be applied. This is not a formal mathematical presentation of statistical methods, or statistical *theory*. Rather, this workshop is a primer that describes how and why particular statistical methods are applied to answer research questions, and how to critically interpret their application and interpretation in the literature.

Although we will not dive into theory or mathematics in detail, you should understand that these methods are thoroughly grounded in mathematics and are not constructed in an *ad hoc* way.

## What You Should Expect From This Workshop

The workshop is presented in a *flipped* format. That is, the homework comes *before* the class, and we use an online Zoom session to work though questions and discussions prompted by the material.

This material is broken up into notebooks (like this one) which you should read. Each notebook should work as a standalone piece of material (though they should be read more-or-less in order). You should attempt the exercises in each notebook and - optionally - also attempt the interactive sections. 

The interactive sessions either fall at the end of a notebook, or are standalone notebooks. Both may require that you install [`RStudio` Desktop](https://rstudio.com/products/rstudio/download/) - a free IDE (Integrated Development Environment) on your own computer. The implementation of statistical methods is handled almost exclusively by computers, these days, and `R` is the most common environment for statistical analysis in scientific research. Gaining some familiarity with `R` will help your research, and maybe even your future employability.

<div id="warning">
**Please be aware that technical questions about `RStudio` etc. will not be addressed in the live online Zoom session**. If you encounter technical difficulties you should email one of the workshop presenters when you encounter the problem.
</div>

You will not be expected to write your own *code* in `R`, only to understand the methods, decide which are appropriate to answer any questions you are given, apply the methods, and interpret the answers you get.

# `R` code used in this notebook

<details>
  <summary>Click to toggle `R` code for Figure 2.1</summary>
  
The `R` code below plots the Normal distribution from figure 2.1, with a random sampling of 200 "measured" variables from this distribution, using `ggplot2`.

```r
mu = 10                          # define mean of distribution
sd = 3                           # define standard deviation of distribution
breaks = seq(0, 20, by=0.4)      # set up the breakpoints between bars in the histogram

dfm_hist = data.frame(vals=rnorm(200, mean=mu, sd=sd))               # create a data frame

p = ggplot(dfm_hist, aes(x=vals))                                    # set up the ggplot with data
p = p + geom_histogram(aes(y=..density..),                           # add a histogram layer
                       breaks=breaks,
                       fill="cornflowerblue")
p = p + stat_function(fun=dnorm, args=list(mean=mu, sd=sd),          # add Normal curve layer
                      geom="line")
p = p + annotate("segment",                                          # show mean as dashed line
                 x=mu, xend=mu, y=0, yend=0.2,
                 colour="darkorange1", size=1, linetype="dashed")
p = p + annotate("segment",                                          # show std devs as dotted lines
                 x=c(mu-2*sd, mu-sd, mu+sd, mu+2*sd),
                 xend=c(mu-2*sd, mu-sd, mu+sd, mu+2*sd),
                 y=c(0, 0, 0, 0), yend=c(0.2, 0.2, 0.2, 0.2),
                 colour="goldenrod", size=1, linetype="dotted")
p = p + annotate("text",                                             # annotate the lines
                 x=c(mu-2*sd, mu-sd, mu, mu+sd, mu+2*sd),
                 y=c(0.21, 0.21, 0.21, 0.21, 0.21),
                 colour="darkorange3",
                 label=c("Âµ - 2Ïƒ", "Âµ - Ïƒ", "Âµ", "Âµ + Ïƒ", "Âµ + 2Ïƒ"))
p = p + xlim(mu - 3 * sd, mu + 3 * sd)                               # set x-axis limits
p = p + xlab("measured variable") + ylab("frequency")                # add axis labels
p = p + theme(plot.background = element_rect(fill = figbg,           # colour background
                                             color = figbg))
p                                                                    # show plot
```
</details>